{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import facebook\n",
    "import requests\n",
    "import csv\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token = \"EAABZBwxwxyXMBAPuUvf6wqDVdJhXcQ5tWZCv42S3UFfZAao935GEou3bFKOPbY2YjOok3r3N6QseA1A1h5OmKJnvizQGWe1DqyBGH494NoGhwxjDD7bRrPTEocSLw2uN1RpPk7TZB5QkQtxJwtbdvQn6Nzw6ZAWDZCD5Li2XCd8QZDZD\"\n",
    "graph = facebook.GraphAPI(access_token=token, version = 2.7)\n",
    "pagesdata = requests.get(\"https://graph.facebook.com/me/accounts?access_token=\"+token)\n",
    "page_id = \"298166859034\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts_on_page = requests.get(\"https://graph.facebook.com/\"+page_id+\"/posts?access_token=\"+token)\n",
    "posts_data = (posts_on_page.json())\n",
    "comments=[]\n",
    "for x in posts_data['data']:\n",
    "    post_id = x['id']\n",
    "    reactions_on_post = requests.get(\"https://graph.facebook.com/\"+post_id+\"/comments?access_token=\"+token)\n",
    "    reactions_data = reactions_on_post.json() \n",
    "    for element in reactions_data['data']:\n",
    "        comments.append(element['message'])\n",
    "#print(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts_on_page = requests.get(\"https://graph.facebook.com/\"+page_id+\"/posts?access_token=\"+token)\n",
    "posts_data = (posts_on_page.json())\n",
    "comments=[]\n",
    "date=[]\n",
    "liste=[]\n",
    "sousliste=[]\n",
    "for x in posts_data['data']:\n",
    "    post_id = x['id']\n",
    "    reactions_on_post = requests.get(\"https://graph.facebook.com/\"+post_id+\"/comments?access_token=\"+token)\n",
    "    reactions_data = reactions_on_post.json() \n",
    "    for element in reactions_data['data']:\n",
    "        sousliste.append(element['message'])\n",
    "        sousliste.append(\"positive\")\n",
    "        sousliste.append(element['created_time'])\n",
    "        sousliste.append(post_id)\n",
    "        liste.extend([sousliste])\n",
    "        sousliste=[]\n",
    "#print(liste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import names\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_vocab = [ 'Ø±Ø§Ø¨Ø­ÙŠÙ†', 'farna','mahlekom' 'fantastic', 'Mabrouk', 'good', 'nice', 'great', ':)' ,'5eir','ðŸ˜˜',' bon','ðŸ˜','satisfied',' Keep it','â¤','ðŸ’•','merci','yo','Ø¨Ø±Ø§ÙÙˆ ','Ø§Ù…Ø´ÙŠØ®ØªÙ†Ø§' ,'Ø§Ù…Ø±Ø¨Ø­ØªÙ†Ø§ ','Ø§Ù…Ø¯Ù„Ù„ØªÙ†Ø§','Ù…Ø¨Ø±ÙˆÙˆÙˆÙƒ']\n",
    "negative_vocab = [ 'Ù‚Ø¯ÙŠÙ…Ø©', 'ØªÙƒØ°Ø¨Ùˆ ','Ø§Ù„Ø®Ø³Ø§Ø±Ù‡', 'hate','Ø³Ø±Ù‚ØªÙˆÙ‡' ':(','kedb ' ,'sirka','Mahouch','Ø¹ÙŠØ¨','ðŸ˜¥','ðŸ˜¢','Ø§Ù„ØªÙØ§Ù‡Ø§Ø§Øª','ðŸ˜­','ðŸ’” ','Ø§Ø³ØªÙ‡ØªØ§Ø±','jboura','inutile','problÃ¨me','Ø§Ù„Ø³Ø±Ù‚Ø©','tay7a']\n",
    "\n",
    " \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "#neutral_features = []\n",
    "\n",
    " \n",
    "train_set = negative_features + positive_features # + neutral_features\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set) \n",
    "\n",
    "posts_on_page = requests.get(\"https://graph.facebook.com/\"+page_id+\"/posts?access_token=\"+token)\n",
    "posts_data = (posts_on_page.json())\n",
    "comments=[]\n",
    "liste =[]\n",
    "for x in posts_data['data']:\n",
    "    post_id = x['id']\n",
    "    reactions_on_post = requests.get(\"https://graph.facebook.com/\"+post_id+\"/comments?access_token=\"+token)\n",
    "    reactions_data = reactions_on_post.json() \n",
    "    for element in reactions_data['data']:\n",
    "        comments.append(element['message'])\n",
    "#print(liste)\n",
    "s_liste_pos=[]\n",
    "liste_pos=[]\n",
    "s_liste_neg=[]\n",
    "liste_neg=[]\n",
    "for i in comments:\n",
    "    neg = 0\n",
    "    pos = 0\n",
    "    #print (i)\n",
    "    i = i.lower()\n",
    "    words = i.split(' ')\n",
    "    for word in words:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'neg':\n",
    "            neg = neg + 1\n",
    "        if classResult == 'pos':\n",
    "            pos = pos + 1\n",
    "        p = float(pos)/len(words)\n",
    "        n = float(neg)/len(words)\n",
    " \n",
    "    #print('Positive: ' + str(p))\n",
    "    #print('Negative: ' + str(n))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import names\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_vocab = [ 'Ø±Ø§Ø¨Ø­ÙŠÙ†', 'farna','mahlekom' 'fantastic', 'Mabrouk', 'good', 'nice', 'great', ':)' ,'5eir','ðŸ˜˜',' bon','ðŸ˜','satisfied',' Keep it','â¤','ðŸ’•','merci','yo','Ø¨Ø±Ø§ÙÙˆ ','Ø§Ù…Ø´ÙŠØ®ØªÙ†Ø§' ,'Ø§Ù…Ø±Ø¨Ø­ØªÙ†Ø§ ','Ø§Ù…Ø¯Ù„Ù„ØªÙ†Ø§','Ù…Ø¨Ø±ÙˆÙˆÙˆÙƒ']\n",
    "negative_vocab = [ 'Ù‚Ø¯ÙŠÙ…Ø©', 'ØªÙƒØ°Ø¨Ùˆ ','Ø§Ù„Ø®Ø³Ø§Ø±Ù‡', 'hate', ':(','kedb ' ,'sirka','Mahouch','Ø¹ÙŠØ¨','ðŸ˜¥','ðŸ˜¢','Ø§Ù„ØªÙØ§Ù‡Ø§Ø§Øª','ðŸ˜­','ðŸ’” ','Ø§Ø³ØªÙ‡ØªØ§Ø±','jboura','inutile','problÃ¨me','Ø§Ù„Ø³Ø±Ù‚Ø©','tay7a']\n",
    "\n",
    " \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "#neutral_features = []\n",
    "\n",
    " \n",
    "train_set = negative_features + positive_features # + neutral_features\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set) \n",
    "\n",
    "posts_on_page = requests.get(\"https://graph.facebook.com/\"+page_id+\"/posts?access_token=\"+token)\n",
    "posts_data = (posts_on_page.json())\n",
    "comments=[]\n",
    "liste =[]\n",
    "for x in posts_data['data']:\n",
    "    post_id = x['id']\n",
    "    reactions_on_post = requests.get(\"https://graph.facebook.com/\"+post_id+\"/comments?access_token=\"+token)\n",
    "    reactions_data = reactions_on_post.json() \n",
    "    for element in reactions_data['data']:\n",
    "        comments.append(element['message'])\n",
    "#print(liste)\n",
    "s_liste_pos=[]\n",
    "liste_pos=[]\n",
    "s_liste_neg=[]\n",
    "liste_neg=[]\n",
    "for i in comments:\n",
    "    neg = 0\n",
    "    pos = 0\n",
    "    #print (i)\n",
    "    i = i.lower()\n",
    "    words = i.split(' ')\n",
    "    for word in words:\n",
    "        s_liste_pos=[]\n",
    "        s_liste_neg=[]\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'neg':\n",
    "            neg = neg + 1\n",
    "        if classResult == 'pos':\n",
    "            pos = pos + 1\n",
    "        p = float(pos)/len(words)\n",
    "        n = float(neg)/len(words) \n",
    "    #print(p)\n",
    "    #print(n)\n",
    "    if(p>n):\n",
    "        s_liste_pos.append(i)\n",
    "        s_liste_pos.append(\"positive\")\n",
    "        liste_pos.extend([s_liste_pos])\n",
    "    if(p<n):\n",
    "        s_liste_neg.append(i)\n",
    "        s_liste_neg.append(\"negative\")\n",
    "        liste_neg.extend([s_liste_neg]) \n",
    "#print(liste_pos)\n",
    "#print(liste_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/sentiment_category_pos.csv\", \"w\", encoding='utf-8') as f_write:\n",
    "    writer = csv.writer(f_write)\n",
    "    for row in liste_pos:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/sentiment_category_neg.csv\", \"w\", encoding='UTF-8') as f_write:\n",
    "    writer = csv.writer(f_write)\n",
    "    for row in liste_neg:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import names\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_vocab = [ 'Ø±Ø§Ø¨Ø­ÙŠÙ†', 'farna','mahlekom' 'fantastic', 'Mabrouk', 'good', 'nice', 'great', ':)' ,'5eir','ðŸ˜˜',' bon','ðŸ˜','satisfied',' Keep it','â¤','ðŸ’•','merci','yo','Ø¨Ø±Ø§ÙÙˆ ','Ø§Ù…Ø´ÙŠØ®ØªÙ†Ø§' ,'Ø§Ù…Ø±Ø¨Ø­ØªÙ†Ø§ ','Ø§Ù…Ø¯Ù„Ù„ØªÙ†Ø§','Ù…Ø¨Ø±ÙˆÙˆÙˆÙƒ']\n",
    "negative_vocab = [ 'Ù‚Ø¯ÙŠÙ…Ø©', 'ØªÙƒØ°Ø¨Ùˆ ','Ø§Ù„Ø®Ø³Ø§Ø±Ù‡', 'hate', ':(','kedb ' ,'sirka','Mahouch','Ø¹ÙŠØ¨','ðŸ˜¥','ðŸ˜¢','Ø§Ù„ØªÙØ§Ù‡Ø§Ø§Øª','ðŸ˜­','ðŸ’” ','Ø§Ø³ØªÙ‡ØªØ§Ø±','jboura','inutile','problÃ¨me','Ø§Ù„Ø³Ø±Ù‚Ø©','tay7a']\n",
    "\n",
    " \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "#neutral_features = []\n",
    "\n",
    " \n",
    "train_set = negative_features + positive_features # + neutral_features\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set) \n",
    "\n",
    "posts_on_page = requests.get(\"https://graph.facebook.com/\"+page_id+\"/posts?access_token=\"+token)\n",
    "posts_data = (posts_on_page.json())\n",
    "comments=[]\n",
    "liste =[]\n",
    "for x in posts_data['data']:\n",
    "    post_id = x['id']\n",
    "    reactions_on_post = requests.get(\"https://graph.facebook.com/\"+post_id+\"/comments?access_token=\"+token)\n",
    "    reactions_data = reactions_on_post.json() \n",
    "    for element in reactions_data['data']:\n",
    "        comments.append(element['message'])\n",
    "#print(liste)\n",
    "s_liste=[]\n",
    "liste1 = []\n",
    "for i in comments:\n",
    "    neg = 0\n",
    "    pos = 0\n",
    "    #print (i)\n",
    "    i = i.lower()\n",
    "    words = i.split(' ')\n",
    "    for word in words:\n",
    "        s_liste=[]\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'neg':\n",
    "            neg = neg + 1\n",
    "        if classResult == 'pos':\n",
    "            pos = pos + 1\n",
    "        p = float(pos)/len(words)\n",
    "        n = float(neg)/len(words) \n",
    "    s_liste.append(i)\n",
    "    if(p>n or p==n):\n",
    "        s_liste.append(\"positive\")\n",
    "        liste1.extend([s_liste])\n",
    "    if(p<n):\n",
    "        s_liste.append(\"negative\")\n",
    "        liste1.extend([s_liste]) \n",
    "#print(liste1)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_on_page = requests.get(\"https://graph.facebook.com/\"+page_id+\"/posts?access_token=\"+token)\n",
    "posts_data = (posts_on_page.json())\n",
    "comments=[]\n",
    "date=[]\n",
    "liste2=[]\n",
    "sousliste=[]\n",
    "for x in posts_data['data']:\n",
    "    post_id = x['id']\n",
    "    reactions_on_post = requests.get(\"https://graph.facebook.com/\"+post_id+\"/comments?access_token=\"+token)\n",
    "    reactions_data = reactions_on_post.json() \n",
    "    for element in reactions_data['data']:\n",
    "        #sousliste.append(element['message'])\n",
    "        #sousliste.append(\"positive\")\n",
    "        sousliste.append(element['created_time'])\n",
    "        sousliste.append(\"https://www.facebook.com/\"+post_id)\n",
    "        liste2.extend([sousliste])\n",
    "        sousliste=[]\n",
    "#print(liste2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = len(liste1)\n",
    "liste3 = [list(i) for i in zip(liste1, liste2)]\n",
    "len(liste3)\n",
    "#liste1.extend(liste2)\n",
    "\n",
    "for i in range(0,p):\n",
    "    liste1[i].extend(liste2[i])\n",
    "    \n",
    "with open(\"data/four-column.csv\", \"w\", encoding='UTF-8') as f_write:\n",
    "    writer = csv.writer(f_write)\n",
    "    for row in liste1:\n",
    "        writer.writerow(row)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = 'dJRBjK8LVPT39mjB7dFwWWJ51'\n",
    "consumer_secret = 'gXRX3WD77ONevSJMlcRBFRWY2AtEiD9BleQLdVfAO4z3GhrsAz'\n",
    "access_token = '3640758196-PtODNLbxszviuEsZJZUuw2Ugw167Q4RvUmVcgHc'\n",
    "access_secret = '9jIxnlnJ8UpH7nsE4aX8FG5prAZ6FlADXFAPlRGpTU4Tv'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)\n",
    "cricTweet = tweepy.Cursor(api.search, q='@OrangeTN').items(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @karim2k: La gÃ©nÃ©rositÃ© de @OrangeTN un voyage au coupÃ© du monde 2018 au fameux CM de @ooredootn Wajih par son prÃ©nom  ðŸ˜„ ðŸ’ŸðŸ’ðŸ’˜ https://t.câ€¦\n",
      "La gÃ©nÃ©rositÃ© de @OrangeTN un voyage au coupÃ© du monde 2018 au fameux CM de @ooredootn Wajih par son prÃ©nom  ðŸ˜„ ðŸ’ŸðŸ’ðŸ’˜ https://t.co/FzrynCCjAN\n",
      "Riles, i like this one too,it was a show!   Thanks to our team and partners @OrangeTN https://t.co/9z3OC3UExq\n",
      "@OrangeTN Pepsi man\n",
      "@OrangeTN_Plus @OrangeTN Bravo !!!\n",
      "RT @millet_thierry: PremiÃ¨re de Riles en Tunisie : de beaux dÃ©buts pour une belle histoire @OrangeTN https://t.co/9ToPqsDDSE\n",
      "@OrangeTN Top ðŸ‘\n",
      "il y a un problÃ¨me d'ADSL @OrangeTN ?\n",
      "PremiÃ¨re de Riles en Tunisie : de beaux dÃ©buts pour une belle histoire @OrangeTN https://t.co/9ToPqsDDSE\n",
      "RT @OrangeTN_Plus: #Solidarity always wins ! Bravo aux bÃ©nÃ©voles d'@OrangeTN et de l'association \"un enfant, des sourires\" pour la rÃ©ussiteâ€¦\n",
      "@OrangeTN sa7a noum\n",
      "RT @OrangeTN_Plus: #Solidarity always wins ! Bravo aux bÃ©nÃ©voles d'@OrangeTN et de l'association \"un enfant, des sourires\" pour la rÃ©ussiteâ€¦\n",
      "@OrangeTN Ø§Ù„Ø§Ø¹Ø¨ Ø§Ø³Ù…Ùˆ wxfdwyzugzqhyifc Ø¨Ù† Ø¨Ø³ØºÙ…Ù‰ÙØ«Ø¶Ø³Ù‡ ÙŠÙ„Ø¹Ø¨ ÙÙŠ Ø§Ù„Ù…Ø§Ù†ÙŠØ§\n",
      "#Solidarity always wins ! Bravo aux bÃ©nÃ©voles d'@OrangeTN et de l'association \"un enfant, des sourires\" pour la rÃ©uâ€¦ https://t.co/vnxFkXPafm\n",
      "@OrangeTN Need for speed\n",
      "@OrangeTN Rayman ðŸ‘\n",
      "RT @elacheche: .@Topnet_FSI &amp; @OrangeTN can you please share a detailed #Technical #PressRelease or #report about the @Huawei #HG532e incidâ€¦\n",
      "Top brands on Twitter in Tunisia: @OrangeTN @Leaders_Tunisie @CLtounes @waleg @lesannoncesTN https://t.co/KCceFL7vU7\n",
      "@OrangeTN I remember I loved the legend of the dragoon !\n",
      "@OrangeTN The Avengers !\n",
      "@OrangeTN GTA\n",
      "@slimKhan @elacheche @OrangeTN @Topnet_FSI @Huawei @WelidNaffati @MedAliSouissi You made my day xD\n",
      "@slimKhan @OrangeTN @Topnet_FSI @Huawei @WelidNaffati @MedAliSouissi That's lovely!! :D ping @WelidNaffati  @MedAliSouissi\n",
      "@elacheche @OrangeTN @Topnet_FSI @Huawei @WelidNaffati @MedAliSouissi xD https://t.co/9hTk1ktnvL\n",
      "@slimKhan @OrangeTN @Topnet_FSI @Huawei @WelidNaffati @MedAliSouissi Seriously?!! :o\n",
      "@OrangeTN @elacheche @Topnet_FSI @Huawei @WelidNaffati @MedAliSouissi W Kamel es2elhom pk ba3D el maj fel orange elâ€¦ https://t.co/G51E8v2IR5\n",
      "@OrangeTN 20/3\n",
      "RT @OrangeTN: Fan d'#HarryPotter et du monde de #Poudlard âœ¨? RÃ©jouissez-vous car un nouveau jeu de rÃ´le sur #mobile #HogwartsMystery sera lâ€¦\n",
      "@OrangeTN Ce matin Ã§a marche comme du tonnerre de Dieu #OrangeTunisie https://t.co/RDOExR58uj\n",
      "RT @elacheche: .@Topnet_FSI &amp; @OrangeTN can you please share a detailed #Technical #PressRelease or #report about the @Huawei #HG532e incidâ€¦\n",
      "@vadmeste @Topnet_FSI @OrangeTN @Huawei @WelidNaffati @MedAliSouissi The default password was already changed by thâ€¦ https://t.co/KDroj4gdh1\n",
      "@OrangeTN le 1150 se moque de moi, jâ€™ai des pbs de connexions, aprÃ¨s mâ€™avoir fait faire changer les cÃ¢bles muraux oâ€¦ https://t.co/OgczCIYbKA\n",
      "@elacheche @Topnet_FSI @OrangeTN @Huawei @WelidNaffati @MedAliSouissi Looks like this one https://t.co/oD1YXzvfic.â€¦ https://t.co/krx4pg9xvY\n",
      ".@Topnet_FSI &amp; @OrangeTN can you please share a detailed #Technical #PressRelease or #report about the @Huaweiâ€¦ https://t.co/mKUR2wQjw7\n",
      "@OrangeTN Ok I will..maybe this weekend haha\n",
      "@OrangeTN 1\n",
      "@OrangeTN la 5\n",
      "vous m'offrez une!\n",
      "@ooredootn impossible de recevoir des sms de validation de pas mal de site internationaux.. par contre sur le rÃ©seaâ€¦ https://t.co/eur92cecHG\n",
      "@BRIsigsauer @OrangeTN @ooredootn c'est pareil dans les campagnes en France, c'est souvent goÃ©graphique mdrr\n",
      "@The_Arabian_Nat @OrangeTN @ooredootn Pourtant la 4G est affichÃ© sur le phone, 2 Minutes pour charger un massage sur Twitter ðŸ˜­\n",
      "@The_Arabian_Nat @OrangeTN @ooredootn 4G inexistante chez ooredoo\n",
      "@OrangeTN @ooredootn De rien. ðŸ˜œ\n",
      "@BRIsigsauer @OrangeTN @ooredootn Mdrrr\n",
      "Si vous allez en Tunisie. un conseil optez pour  @OrangeTN dans le sud, @ooredootn câ€™est vraiment pire que free en FranceðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­\n",
      "@OrangeTN Mon souci, câ€™est vos textos de pub reÃ§u trois fois par semaine.\n",
      "@OrangeTN @MBLAMINE @ooredootn Merci :)\n",
      "@MBLAMINE @OrangeTN @ooredootn ðŸ˜‘\n",
      "@Molka_Chaari @OrangeTN Ya que la boutique @ooredootn au cas oÃ¹ ;)\n",
      "@OrangeTN y a une boutique orange au Tunisia Mall?\n",
      "Ca va @OrangeTN? Je ne vous dÃ©range pas trop? #harcelementpublicitaire https://t.co/Zv30gocPGL\n",
      "@Karim_elo @OrangeTN Ã§a dÃ©pend toujours de votre utilisation et non pas de la validitÃ© du forfait, mais si voulez oâ€¦ https://t.co/vJxF24aTUz\n",
      "DurÃ©e du forfait donnÃ©es mobiles 1Go en Tunisie : avec @OrangeTN 10 jours, avec @ooredootn 2 jours...et sans changeâ€¦ https://t.co/OKxfRX8ePX\n",
      "@OrangeTN Didn't watch any\n",
      "@OrangeTN D'accord merci\n",
      "So kifech taw @OrangeTN tjewebech3al les tweet service client ma7abech totlob w 3 semaine sans adsl Â¯\\_(ãƒ„)_/Â¯\n",
      "@OrangeTN On paie 39d chaque mois??\n",
      "@OrangeTN On paie 39 d chaque mois??\n",
      "RT @Marwabek_tn: Bravo les gars ðŸ‘ðŸ»ðŸ‘ðŸ»@OrangeTN Handball ðŸ¤¾â€â™€ï¸ team ! https://t.co/AukUIaYMXl\n",
      "@OrangeTN Iphone b 200dt !! Impossible !\n",
      "RT @Marwabek_tn: Bravo les gars ðŸ‘ðŸ»ðŸ‘ðŸ»@OrangeTN Handball ðŸ¤¾â€â™€ï¸ team ! https://t.co/AukUIaYMXl\n"
     ]
    }
   ],
   "source": [
    "tweets=[]\n",
    "for tweet in cricTweet:\n",
    "    print(tweet.text)\n",
    "    tweets.append(tweet.text)\n",
    "    \n",
    "with open('comments.csv', 'a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for val in tweets:\n",
    "        writer.writerow([val])\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
